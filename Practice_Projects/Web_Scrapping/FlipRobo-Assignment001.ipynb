{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5faa1da3",
   "metadata": {},
   "source": [
    "# ASSIGNMENT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5c878",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fea842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main Page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Navigation menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Personal tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Namespaces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Views</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Navigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Print/export</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>In other projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Header Tags\n",
       "0                       Main Page\n",
       "1            Welcome to Wikipedia\n",
       "2   From today's featured article\n",
       "3                Did you know ...\n",
       "4                     In the news\n",
       "5                     On this day\n",
       "6        Today's featured picture\n",
       "7        Other areas of Wikipedia\n",
       "8     Wikipedia's sister projects\n",
       "9             Wikipedia languages\n",
       "10                Navigation menu\n",
       "11                 Personal tools\n",
       "12                     Namespaces\n",
       "13                          Views\n",
       "14                     Navigation\n",
       "15                     Contribute\n",
       "16                          Tools\n",
       "17                   Print/export\n",
       "18              In other projects\n",
       "19                      Languages"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display all the header tags from wikipedia.org.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def wikipedia(url):\n",
    "    #creating request object\n",
    "    pageli=requests.get(url)\n",
    "    \n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(pageli.content)\n",
    "    \n",
    "    # list with header tags\n",
    "    list=['h1','h2','h3']\n",
    "    \n",
    "    # get header tag details\n",
    "    li=[]\n",
    "    for i in soup.find_all(list):\n",
    "        li.append(i.text.strip())\n",
    "    # dataframe with header tag details\n",
    "    df=pd.DataFrame(li,columns=['Header Tags'])\n",
    "    return df\n",
    "    \n",
    "wikipedia(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b09e8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Le fabuleux destin d'Amélie Poulain</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Kid</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name rating year of release\n",
       "0              The Shawshank Redemption    9.3            1994\n",
       "1                         The Godfather    9.2            1972\n",
       "2                 The Godfather Part II      9            1974\n",
       "3                       The Dark Knight      9            2008\n",
       "4                          12 Angry Men      9            1957\n",
       "..                                  ...    ...             ...\n",
       "95                   North by Northwest    8.3            1959\n",
       "96                   A Clockwork Orange    8.3            1971\n",
       "97                               Snatch    8.2            2000\n",
       "98  Le fabuleux destin d'Amélie Poulain    8.3            2001\n",
       "99                              The Kid    8.3            1921\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def imdbtop100(url):\n",
    "    #creating request object\n",
    "    movielink=requests.get(url)\n",
    "    \n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(movielink.content)\n",
    "    \n",
    "    # Get name details\n",
    "    titles=[]\n",
    "    arr=[]\n",
    "    kk=0\n",
    "    for i in soup.find_all('h3',class_=\"lister-item-header\"):\n",
    "        titles.append(i.text.strip())\n",
    "        ts=titles[kk].split()\n",
    "        td=ts[1:-1]\n",
    "        tf=\" \".join(td)\n",
    "        arr.append(tf)\n",
    "        kk=kk+1\n",
    "    \n",
    "    # get year of release details\n",
    "    year=[]\n",
    "    for i in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "        year.append(i.text[1:-1])\n",
    "    \n",
    "    # get rating details\n",
    "    rating=[]\n",
    "    for j in soup.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "        rating.append(j.text.strip())\n",
    "    \n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({\"name\":arr,\"rating\":rating,\"year of release\":year})\n",
    "    return df\n",
    "\n",
    "imdbtop100(\"https://www.imdb.com/list/ls091520106/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e8bcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Kaakkaa Muttai</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Theeran Adhigaaram Ondru</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name rating year of release\n",
       "0   Ramayana: The Legend of Prince Rama    8.5            1993\n",
       "1            Rocketry: The Nambi Effect    8.4            2022\n",
       "2                               Golmaal    8.4            1979\n",
       "3                           777 Charlie    8.4            2022\n",
       "4                               Nayakan    8.4            1987\n",
       "..                                  ...    ...             ...\n",
       "95                       Kaakkaa Muttai    8.0            2014\n",
       "96                          Ustad Hotel    8.0            2012\n",
       "97             Theeran Adhigaaram Ondru    8.0            2017\n",
       "98                               Angoor    8.0            1982\n",
       "99                      Rang De Basanti    8.0            2006\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def imdbtop100IM(url):\n",
    "    \n",
    "    #creating request object\n",
    "    movielink=requests.get(url)\n",
    "    \n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(movielink.content)\n",
    "    \n",
    "    # get name details\n",
    "    titles=[]\n",
    "    arr=[]\n",
    "    kk=0\n",
    "    for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "        titles.append(i.text.strip())\n",
    "        ts=titles[kk].split()\n",
    "        td=ts[1:-1]\n",
    "        tf=\" \".join(td)\n",
    "        arr.append(tf)\n",
    "        kk=kk+1\n",
    "        if(kk==100):\n",
    "            break\n",
    "    # print(len(arr))\n",
    "    \n",
    "    # get year of release details\n",
    "    year=[]\n",
    "    kk=0\n",
    "    for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "        year.append(i.text[1:-1])\n",
    "        kk=kk+1\n",
    "        if(kk==100):\n",
    "            break\n",
    "        # print(len(year))\n",
    "    \n",
    "    #get rating details\n",
    "    rating=[]\n",
    "    kk=0\n",
    "    for j in soup.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
    "        rating.append(j.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==100):\n",
    "            break\n",
    "        # print(len(rating))\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({\"name\":arr,\"rating\":rating,\"year of release\":year})\n",
    "    return df\n",
    "\n",
    "imdbtop100IM(\"https://www.imdb.com/india/top-rated-indian-movies/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c657260e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PresidentiaL List</th>\n",
       "      <th>Term of office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PresidentiaL List  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of office  \n",
       "0                      25 July, 2017 to 25 July, 2022  \n",
       "1                      25 July, 2012 to 25 July, 2017  \n",
       "2                      25 July, 2007 to 25 July, 2012  \n",
       "3                      25 July, 2002 to 25 July, 2007  \n",
       "4                      25 July, 1997 to 25 July, 2002  \n",
       "5                      25 July, 1992 to 25 July, 1997  \n",
       "6                      25 July, 1987 to 25 July, 1992  \n",
       "7                      25 July, 1982 to 25 July, 1987  \n",
       "8                      25 July, 1977 to 25 July, 1982  \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) \n",
    "#from https://presidentofindia.nic.in/former-presidents.htm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def profIND(url):\n",
    "    #creating request object\n",
    "    presdlink=requests.get(url)\n",
    "    \n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(presdlink.content)\n",
    "    \n",
    "    # Scrapping names\n",
    "    titles=[]\n",
    "    arr=[]\n",
    "    kk=0\n",
    "    for i in soup.find_all('h3'):\n",
    "        titles.append(i.text.strip())\n",
    "        ts=titles[kk].split(\"(\")\n",
    "        td=ts[0:-1]\n",
    "        tf=\" \".join(td)\n",
    "        arr.append(tf)\n",
    "        kk=kk+1\n",
    "    \n",
    "    # Scrapping terms of office\n",
    "    Troffice=[]\n",
    "    for i in soup.find_all('p'):\n",
    "        Troffice.append(i.text)\n",
    "    #print(Troffice)\n",
    "    result_1 = [item.split(':') for item in Troffice]\n",
    "    result_2 = [item for l in result_1 for item in l]\n",
    "    result_3=[item.strip(\" \") for item in result_2]\n",
    "    result_4=[item.split(\" \") for item in result_3]\n",
    "    result_5=[item for item in result_4 if len(item)==7 or len(item)==15 ]\n",
    "    Troffice=[]\n",
    "    for i in result_5:\n",
    "        result_6=\" \".join(i)\n",
    "        Troffice.append(result_6)\n",
    "    # print(len(Troffice))\n",
    "    # print(len(arr))\n",
    "    \n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'PresidentiaL List':arr,'Term of office':Troffice})\n",
    "    return df\n",
    "\n",
    "profIND(\"https://presidentofindia.nic.in/former-presidents.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6e6862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Country</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India</td>\n",
       "      <td>35</td>\n",
       "      <td>3,866</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>30</td>\n",
       "      <td>2,677</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>19</td>\n",
       "      <td>1,380</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank       Country Matches Points Rating\n",
       "0    1   New Zealand      23  2,670    116\n",
       "1    2       England      30  3,400    113\n",
       "2    3     Australia      32  3,572    112\n",
       "3    4         India      35  3,866    110\n",
       "4    5      Pakistan      22  2,354    107\n",
       "5    6  South Africa      24  2,392    100\n",
       "6    7    Bangladesh      30  2,753     92\n",
       "7    8     Sri Lanka      30  2,677     89\n",
       "8    9   Afghanistan      19  1,380     73\n",
       "9   10   West Indies      41  2,902     71"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#https://www.icc-cricket.com/rankings/mens/team-rankings/odi\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def topodimpr(url):\n",
    "    \n",
    "    #creating request object\n",
    "    womelink=requests.get(url)\n",
    "    \n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "    \n",
    "    # scrapping rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('td',class_=\"rankings-block__banner--pos\")\n",
    "    Rank.append(RR.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell table-body__cell--position u-text-right\"):\n",
    "        Rank.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "            \n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    country=[]\n",
    "    for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "        country.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==10):\n",
    "            break\n",
    "    \n",
    "    # scrapping matches details\n",
    "    kk=0\n",
    "    matches=[]\n",
    "    mm=soup.find('td',class_=\"rankings-block__banner--matches\")\n",
    "    matches.append(mm.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "        if(kk%2==0):\n",
    "            matches.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==18):\n",
    "            break\n",
    "    \n",
    "    # scrapping points details\n",
    "    kk=0\n",
    "    points=[]\n",
    "    pp=soup.find('td',class_=\"rankings-block__banner--points\")\n",
    "    points.append(pp.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "        if(kk%2!=0):\n",
    "            points.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==18):\n",
    "            break\n",
    "    \n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Rating=[]\n",
    "    rr=soup.find('td',class_=\"rankings-block__banner--rating u-text-right\")\n",
    "    Rating.append(rr.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "        Rating.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "    \n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Country':country,'Matches':matches,'Points':points, 'Rating':Rating})\n",
    "    return df\n",
    "\n",
    "topodimpr(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ded368c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank            Player Name Country Rating\n",
       "0    1             Babar Azam     PAK    890\n",
       "1    2            Imam-ul-Haq     PAK    779\n",
       "2    3  Rassie van der Dussen      SA    766\n",
       "3    4        Quinton de Kock      SA    759\n",
       "4    5           David Warner     AUS    747\n",
       "5    6            Steve Smith     AUS    719\n",
       "6    7         Jonny Bairstow     ENG    710\n",
       "7    8            Virat Kohli     IND    707\n",
       "8    9           Rohit Sharma     IND    704\n",
       "9   10        Kane Williamson      NZ    701"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "#https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def topoditr(url):\n",
    "\n",
    "    # creating requests object\n",
    "    womelink=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "\n",
    "    # scrapping rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('span',class_=\"rankings-block__pos-number\")\n",
    "    Rank.append(RR.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        Rank.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping player name details\n",
    "    kk=0\n",
    "    Name=[]\n",
    "    nn=soup.find('div',class_=\"rankings-block__banner--name-large\")\n",
    "    Name.append(nn.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Name.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    Country=[]\n",
    "    cc=soup.find('div',class_=\"rankings-block__banner--nationality\")\n",
    "    Country.append(cc.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        Country.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Score=[]\n",
    "    rt=soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "    Score.append(rt.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        Score.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    #dataframe with all the details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Player Name':Name,'Country':Country,'Rating':Score})\n",
    "    return df\n",
    "\n",
    "topoditr(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce15b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank        Player Name Country Rating\n",
       "0    1        Trent Boult      NZ    760\n",
       "1    2     Josh Hazlewood     AUS    727\n",
       "2    3     Mitchell Starc     AUS    665\n",
       "3    4     Shaheen Afridi     PAK    661\n",
       "4    5         Matt Henry      NZ    656\n",
       "5    6         Adam Zampa     AUS    655\n",
       "6    7       Mehedi Hasan     BAN    655\n",
       "7    8   Mujeeb Ur Rahman     AFG    650\n",
       "8    9  Mustafizur Rahman     BAN    640\n",
       "9   10        Rashid Khan     AFG    635"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "#https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def topodibtr(url):\n",
    "\n",
    "    #creating requests object\n",
    "    womelink=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "\n",
    "    # scrapping rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('span',class_=\"rankings-block__pos-number\")\n",
    "    Rank.append(RR.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        Rank.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping player name details\n",
    "    kk=0\n",
    "    Name=[]\n",
    "    nn=soup.find('div',class_=\"rankings-block__banner--name-large\")\n",
    "    Name.append(nn.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Name.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    Country=[]\n",
    "    cc=soup.find('div',class_=\"rankings-block__banner--nationality\")\n",
    "    Country.append(cc.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        Country.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Score=[]\n",
    "    rt=soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "    Score.append(rt.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        Score.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Player Name':Name,'Country':Country,'Rating':Score})\n",
    "    return df\n",
    "\n",
    "topodibtr(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5c0a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Country</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>3,061</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>England</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Indies</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>8</td>\n",
       "      <td>572</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>24</td>\n",
       "      <td>1,519</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank       Country Matches Points Rating\n",
       "0    1     Australia      18  3,061    170\n",
       "1    2  South Africa      26  3,098    119\n",
       "2    3       England      25  2,904    116\n",
       "3    4         India      27  2,820    104\n",
       "4    5   New Zealand      24  2,425    101\n",
       "5    6   West Indies      24  2,334     97\n",
       "6    7    Bangladesh      12    932     78\n",
       "7    8      Thailand       8    572     72\n",
       "8    9      Pakistan      24  1,519     63\n",
       "9   10     Sri Lanka       8    353     44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#https://www.icc-cricket.com/rankings/womens/team-rankings/odi\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def topwmpr(url):\n",
    "\n",
    "    # creating requests object\n",
    "    womelink=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "\n",
    "    # scrapping Rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('td',class_=\"rankings-block__banner--pos\")\n",
    "    Rank.append(RR.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell table-body__cell--position u-text-right\"):\n",
    "        Rank.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    country=[]\n",
    "    for i in soup.find_all('span',class_=\"u-hide-phablet\"):\n",
    "        country.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==10):\n",
    "            break\n",
    "\n",
    "    # scrapping matches details\n",
    "    kk=0\n",
    "    matches=[]\n",
    "    mm=soup.find('td',class_=\"rankings-block__banner--matches\")\n",
    "    matches.append(mm.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "        if(kk%2==0):\n",
    "            matches.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==18):\n",
    "            break\n",
    "        \n",
    "    # scrapping points details\n",
    "    kk=0\n",
    "    points=[]\n",
    "    pp=soup.find('td',class_=\"rankings-block__banner--points\")\n",
    "    points.append(pp.text)\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "        if(kk%2!=0):\n",
    "            points.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==18):\n",
    "            break\n",
    "\n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Rating=[]\n",
    "    rr=soup.find('td',class_=\"rankings-block__banner--rating u-text-right\")\n",
    "    Rating.append(rr.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell u-text-right rating\"):\n",
    "        Rating.append(i.text)\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Country':country,'Matches':matches,'Points':points, 'Rating':Rating})\n",
    "    return df\n",
    "\n",
    "topwmpr(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bced21b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank          Player Name Country Rating\n",
       "0    1         Alyssa Healy     AUS    785\n",
       "1    2          Beth Mooney     AUS    749\n",
       "2    3      Laura Wolvaardt      SA    732\n",
       "3    4       Natalie Sciver     ENG    725\n",
       "4    5     Harmanpreet Kaur     IND    716\n",
       "5    6      Smriti Mandhana     IND    714\n",
       "6    7          Meg Lanning     AUS    710\n",
       "7    8       Rachael Haynes     AUS    701\n",
       "8    9    Amy Satterthwaite      NZ    661\n",
       "9   10  Chamari Athapaththu      SL    655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def topodiwtr(url):\n",
    "    \n",
    "    # creating requests object\n",
    "    womelink=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "\n",
    "    # scrapping rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('span',class_=\"rankings-block__pos-number\")\n",
    "    Rank.append(RR.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        Rank.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping player name details\n",
    "    kk=0\n",
    "    Name=[]\n",
    "    nn=soup.find('div',class_=\"rankings-block__banner--name-large\")\n",
    "    Name.append(nn.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Name.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    Country=[]\n",
    "    cc=soup.find('div',class_=\"rankings-block__banner--nationality\")\n",
    "    Country.append(cc.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        Country.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Score=[]\n",
    "    rt=soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "    Score.append(rt.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        Score.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Player Name':Name,'Country':Country,'Rating':Score})\n",
    "    return df\n",
    "\n",
    "topodiwtr(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed76435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank       Player Name Country Rating\n",
       "0    1   Hayley Matthews      WI    380\n",
       "1    2      Ellyse Perry     AUS    374\n",
       "2    3    Natalie Sciver     ENG    357\n",
       "3    4       Amelia Kerr      NZ    356\n",
       "4    5    Marizanne Kapp      SA    349\n",
       "5    6     Deepti Sharma     IND    322\n",
       "6    7  Ashleigh Gardner     AUS    270\n",
       "7    8     Jess Jonassen     AUS    246\n",
       "8    9    Jhulan Goswami     IND    214\n",
       "9   10   Katherine Brunt     ENG    207"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "#https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def topodiwtra(url):\n",
    "\n",
    "    # crating requests object\n",
    "    womelink=requests.get(url)\n",
    "\n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(womelink.content)\n",
    "\n",
    "    # scrapping rank details\n",
    "    kk=0\n",
    "    Rank=[]\n",
    "    RR=soup.find('span',class_=\"rankings-block__pos-number\")\n",
    "    Rank.append(RR.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"rankings-table__pos-number\"):\n",
    "        Rank.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping player name details\n",
    "    kk=0\n",
    "    Name=[]\n",
    "    nn=soup.find('div',class_=\"rankings-block__banner--name-large\")\n",
    "    Name.append(nn.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rankings-table__name name\"):\n",
    "        Name.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping country details\n",
    "    kk=0\n",
    "    Country=[]\n",
    "    cc=soup.find('div',class_=\"rankings-block__banner--nationality\")\n",
    "    Country.append(cc.text.strip())\n",
    "    for i in soup.find_all('span',class_=\"table-body__logo-text\"):\n",
    "        Country.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "        \n",
    "    # scrapping rating details\n",
    "    kk=0\n",
    "    Score=[]\n",
    "    rt=soup.find('div',class_=\"rankings-block__banner--rating\")\n",
    "    Score.append(rt.text.strip())\n",
    "    for i in soup.find_all('td',class_=\"table-body__cell rating\"):\n",
    "        Score.append(i.text.strip())\n",
    "        kk=kk+1\n",
    "        if(kk==9):\n",
    "            break\n",
    "\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Rank':Rank,'Player Name':Name,'Country':Country,'Rating':Score})\n",
    "    return df\n",
    "\n",
    "topodiwtra(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "758e7669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manufacturing orders from China down 40% in de...</td>\n",
       "      <td>4 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/manufacturing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Death of the internal combustion engine — Cowe...</td>\n",
       "      <td>37 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/death-of-the-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These stocks are cheap heading into 2023, and ...</td>\n",
       "      <td>42 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/these-stocks-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who will be Disney's next CEO? Here are the to...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/disney-ceo-top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Study: exercise may increase the effectiveness...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/study-exercise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OPEC+ agrees to stick to its existing policy o...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/opec-meeting-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parking lots are becoming as important as cars...</td>\n",
       "      <td>22 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/parking-lots-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amazon's cloud unit faces cost-sensitive custo...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/aws-faces-cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Don’t overlook this health warning on your dec...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/dont-overlook-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How electric air taxis could shake up the airl...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/how-electric-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I raised 2 successful CEOs and a doctor. Here'...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/i-raised-2-suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Delta pilots would get more than 30% in pay ra...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/delta-pilots-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Men participate less in 401(k) plans than wome...</td>\n",
       "      <td>24 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/men-participat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The best U.S. states to raise a family if you ...</td>\n",
       "      <td>24 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/best-states-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Susan Cain: This Bob Dylan-inspired phrase can...</td>\n",
       "      <td>24 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/bestselling-au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The difference between this comeback and the m...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/the-difference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Goldman says buy these five stocks for the lon...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/goldman-says-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Celsius users with crypto collateral stuck tur...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/celsius-users-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cramer's lightning round: Let Extreme Networks...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jim Cramer says these 3 apparel stocks benefit...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cramer’s week ahead: Markets need a strong job...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>There is 'enormous opportunity' in REITs, says...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/reits-offer-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Biden administration will end monkeypox public...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Expect more choppiness ahead after a week of m...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/expect-more-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GM, LG investing $275 million to expand Tennes...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/gm-lg-investin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>These beat-up tech stocks have potential, ‘Hal...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/big-tech-stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The Fed's path to a 'Goldilocks' economy just ...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/the-feds-path-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3 things crypto investors need to know in post...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/three-things-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Biden condemns antisemitism as Ye praises Hitl...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-condemns...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline              Time  \\\n",
       "0   Manufacturing orders from China down 40% in de...         4 Min Ago   \n",
       "1   Death of the internal combustion engine — Cowe...        37 Min Ago   \n",
       "2   These stocks are cheap heading into 2023, and ...        42 Min Ago   \n",
       "3   Who will be Disney's next CEO? Here are the to...        55 Min Ago   \n",
       "4   Study: exercise may increase the effectiveness...        55 Min Ago   \n",
       "5   OPEC+ agrees to stick to its existing policy o...        1 Hour Ago   \n",
       "6   Parking lots are becoming as important as cars...      22 Hours Ago   \n",
       "7   Amazon's cloud unit faces cost-sensitive custo...      23 Hours Ago   \n",
       "8   Don’t overlook this health warning on your dec...      23 Hours Ago   \n",
       "9   How electric air taxis could shake up the airl...      23 Hours Ago   \n",
       "10  I raised 2 successful CEOs and a doctor. Here'...      23 Hours Ago   \n",
       "11  Delta pilots would get more than 30% in pay ra...      23 Hours Ago   \n",
       "12  Men participate less in 401(k) plans than wome...      24 Hours Ago   \n",
       "13  The best U.S. states to raise a family if you ...      24 Hours Ago   \n",
       "14  Susan Cain: This Bob Dylan-inspired phrase can...      24 Hours Ago   \n",
       "15  The difference between this comeback and the m...  December 3, 2022   \n",
       "16  Goldman says buy these five stocks for the lon...  December 3, 2022   \n",
       "17  Celsius users with crypto collateral stuck tur...  December 3, 2022   \n",
       "18  Cramer's lightning round: Let Extreme Networks...  December 2, 2022   \n",
       "19  Jim Cramer says these 3 apparel stocks benefit...  December 2, 2022   \n",
       "20  Cramer’s week ahead: Markets need a strong job...  December 2, 2022   \n",
       "21  Pro Picks: Watch all of Friday's big stock cal...  December 2, 2022   \n",
       "22  There is 'enormous opportunity' in REITs, says...  December 2, 2022   \n",
       "23  Biden administration will end monkeypox public...  December 2, 2022   \n",
       "24  Expect more choppiness ahead after a week of m...  December 2, 2022   \n",
       "25  GM, LG investing $275 million to expand Tennes...  December 2, 2022   \n",
       "26  These beat-up tech stocks have potential, ‘Hal...  December 2, 2022   \n",
       "27  The Fed's path to a 'Goldilocks' economy just ...  December 2, 2022   \n",
       "28  3 things crypto investors need to know in post...  December 2, 2022   \n",
       "29  Biden condemns antisemitism as Ye praises Hitl...  December 2, 2022   \n",
       "\n",
       "                                            News_Link  \n",
       "0   https://www.cnbc.com/2022/12/04/manufacturing-...  \n",
       "1   https://www.cnbc.com/2022/12/04/death-of-the-i...  \n",
       "2   https://www.cnbc.com/2022/12/04/these-stocks-a...  \n",
       "3   https://www.cnbc.com/2022/12/04/disney-ceo-top...  \n",
       "4   https://www.cnbc.com/2022/12/04/study-exercise...  \n",
       "5   https://www.cnbc.com/2022/12/04/opec-meeting-o...  \n",
       "6   https://www.cnbc.com/2022/12/03/parking-lots-b...  \n",
       "7   https://www.cnbc.com/2022/12/03/aws-faces-cost...  \n",
       "8   https://www.cnbc.com/2022/12/03/dont-overlook-...  \n",
       "9   https://www.cnbc.com/2022/12/03/how-electric-a...  \n",
       "10  https://www.cnbc.com/2022/12/03/i-raised-2-suc...  \n",
       "11  https://www.cnbc.com/2022/12/03/delta-pilots-w...  \n",
       "12  https://www.cnbc.com/2022/12/03/men-participat...  \n",
       "13  https://www.cnbc.com/2022/12/03/best-states-ra...  \n",
       "14  https://www.cnbc.com/2022/12/03/bestselling-au...  \n",
       "15  https://www.cnbc.com/2022/12/03/the-difference...  \n",
       "16  https://www.cnbc.com/2022/12/03/goldman-says-b...  \n",
       "17  https://www.cnbc.com/2022/12/03/celsius-users-...  \n",
       "18  https://www.cnbc.com/2022/12/02/cramers-lightn...  \n",
       "19  https://www.cnbc.com/2022/12/02/jim-cramer-say...  \n",
       "20  https://www.cnbc.com/2022/12/02/cramers-week-a...  \n",
       "21  https://www.cnbc.com/2022/12/02/pro-picks-watc...  \n",
       "22  https://www.cnbc.com/2022/12/02/reits-offer-en...  \n",
       "23  https://www.cnbc.com/2022/12/02/biden-administ...  \n",
       "24  https://www.cnbc.com/2022/12/02/expect-more-ch...  \n",
       "25  https://www.cnbc.com/2022/12/02/gm-lg-investin...  \n",
       "26  https://www.cnbc.com/2022/12/02/big-tech-stock...  \n",
       "27  https://www.cnbc.com/2022/12/02/the-feds-path-...  \n",
       "28  https://www.cnbc.com/2022/12/02/three-things-c...  \n",
       "29  https://www.cnbc.com/2022/12/02/biden-condemns...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "# i)Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def cnbchtn(url):\n",
    "\n",
    "    # creating requests object\n",
    "    url1=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(url1.content)\n",
    "\n",
    "    # scrapping headline details\n",
    "    Headline=[]\n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        Headline.append(i.text.strip())\n",
    "\n",
    "    # scrapping time details\n",
    "    Time=[]\n",
    "    for i in soup.find_all('time',class_=\"LatestNews-timestamp\"):\n",
    "        Time.append(i.text.strip())\n",
    "\n",
    "    # scrapping newslink details\n",
    "    News_Link=[]    \n",
    "    tot_data = soup.find_all('li', {'class':\"LatestNews-item\"})\n",
    "    for sub_data in tot_data:\n",
    "        attr_tags = sub_data.find_all('a', href=True) #  Find all <a> tags that have a href attribute\n",
    "        for tag in attr_tags:\n",
    "            Newslink=tag['href']\n",
    "        News_Link.append(Newslink)\n",
    "\n",
    "    # dataframe with all details\n",
    "    df=pd.DataFrame({'Headline':Headline,'Time':Time,'News_Link':News_Link})\n",
    "    return df\n",
    "\n",
    "cnbchtn(\"https://www.cnbc.com/world/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266640fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                               Author  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                    Prakken, Henry, Sartor, Giovanni    October 2015   \n",
       "3                                  Boden, Margaret A.     August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                         Miller, Tim   February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11                Bench-Capon, T.J.M., Dunne, Paul E.    October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                       Blum, Avrim L., Langley, Pat   December 1997   \n",
       "15                    Arora, Saurabh, Doshi, Prashant     August 2021   \n",
       "16       Aas, Kjersti, Jullum, Martin, Løland, Anders  September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18     Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.   December 2016   \n",
       "19                       Riveiro, Maria, Thill, Serge  September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                       Kohavi, Ron, John, George H.   December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                    Ying, Mingsheng   February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "# https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# Scrape below mentioned details :\n",
    "# i) Paper Title \n",
    "# ii) Authors\n",
    "# iii) Published Date \n",
    "# iv) Paper URL\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def aiarticles(url):\n",
    "    \n",
    "    # creating requests object\n",
    "    url1=requests.get(url)\n",
    "\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(url1.content)\n",
    "\n",
    "    # scrapping paper titles\n",
    "    Title=[]\n",
    "    for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        Title.append(i.text.strip())\n",
    "\n",
    "    # scrapping author details\n",
    "    Author=[]\n",
    "    for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        Author.append(i.text.strip())\n",
    "\n",
    "    # scrapping published date details\n",
    "    PDate=[]\n",
    "    for i in soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "        PDate.append(i.text.strip())\n",
    "\n",
    "    # scrapping Paper URL details\n",
    "    Paper_URL=[]    \n",
    "    tot_data = soup.find_all('li', {'class':\"sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs\"})\n",
    "    for data1 in tot_data:\n",
    "        att_tags = data1.find_all('a', href=True) #  Find all <a> tags that have a href attr\n",
    "        for tag in att_tags:\n",
    "            Plink=tag['href']\n",
    "        Paper_URL.append(Plink)\n",
    "    \n",
    "    # Dataframe with all details\n",
    "    df=pd.DataFrame({'Paper Title':Title,'Author':Author,'Published Date':PDate,'Paper URL':Paper_URL})\n",
    "    return df\n",
    "\n",
    "aiarticles(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5d3a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant name                       Cuisine  \\\n",
       "0                   Castle Barbeque         Chinese, North Indian   \n",
       "1                   Jungle Jamboree  North Indian, Asian, Italian   \n",
       "2                   Castle Barbeque         Chinese, North Indian   \n",
       "3                        Cafe Knosh          Italian, Continental   \n",
       "4              The Barbeque Company         North Indian, Chinese   \n",
       "5                       India Grill         North Indian, Italian   \n",
       "6                    Delhi Barbeque                  North Indian   \n",
       "7  The Monarch - Bar Be Que Village                  North Indian   \n",
       "8                 Indian Grill Room         North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.6   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in :\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location \n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def dineout(url):\n",
    "\n",
    "    # creating request object\n",
    "    url1=requests.get(url)\n",
    "\n",
    "    #creating soup object\n",
    "    soup=BeautifulSoup(url1.content)\n",
    "\n",
    "    # scrap restuarant details\n",
    "    RName=[]\n",
    "    for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        RName.append(i.text.strip())\n",
    "\n",
    "    # scrap cuisine details\n",
    "    Cuisine0=[]\n",
    "    for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        Cuisine0.append(i.text.strip())\n",
    "    result_1 = [item.split('|') for item in Cuisine0]\n",
    "    result_2 = [item for l in result_1 for item in l]\n",
    "    result_3=[item.strip(\" \") for item in result_2]\n",
    "    Cuisine=[]\n",
    "    kk=0\n",
    "    for item in result_3:\n",
    "        if(kk%2!=0):\n",
    "            Cuisine.append(item)\n",
    "        kk=kk+1\n",
    "\n",
    "    # scrap location details\n",
    "    Location=[]\n",
    "    for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        Location.append(i.text.strip())\n",
    "    \n",
    "    # scrap ratings details\n",
    "    Ratings=[]\n",
    "    for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "        Ratings.append(i.text.strip())\n",
    "    \n",
    "    Images=[]\n",
    "    for i in soup.find_all(\"img\",class_=\"no-img\"):\n",
    "        Images.append(i.get('data-src'))\n",
    "\n",
    "    df=pd.DataFrame({'Restaurant name':RName,'Cuisine':Cuisine,'Location':Location,'Ratings':Ratings,'Image URL':Images})\n",
    "    return df\n",
    "\n",
    "dineout(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cfdd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape the details of top publications from Google Scholar from \n",
    "# https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "# i) Rank \n",
    "# ii) Publication\n",
    "# iii) h5-index\n",
    "#  iv) h5-median\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def tpgoogsch(url):\n",
    "\n",
    "    # creating request object\n",
    "    url1=requests.get(url)\n",
    "    # creating soup object\n",
    "    soup=BeautifulSoup(url1.content)\n",
    "\n",
    "    # scrape rank details\n",
    "    Rank=[]\n",
    "    for i in soup.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "        Rank.append(i.text.strip())\n",
    "\n",
    "    # scrap pulblicaton details    \n",
    "    Publication=[]\n",
    "    for i in soup.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "        Publication.append(i.text.strip())\n",
    "\n",
    "    # scrap h5_index details\n",
    "    h5_index=[]\n",
    "    for i in soup.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_index.append(i.text.strip())\n",
    "    \n",
    "    # scrap h5_median details\n",
    "    h5_median=[]\n",
    "    for i in soup.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_median.append(i.text.strip())\n",
    "\n",
    "    # create data frame with all the data\n",
    "    df=pd.DataFrame({'Rank':Rank,'Publication':Publication,'h5-index':h5_index,'h5-median':h5_median})\n",
    "    return df\n",
    "\n",
    "tpgoogsch(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
